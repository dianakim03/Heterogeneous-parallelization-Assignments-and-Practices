# Часть 2: Параллельная очередь FIFO на GPU (CUDA)

Во второй части лабораторной работы реализована параллельная очередь (FIFO) на GPU с использованием глобальной памяти и атомарных операций.  
Основная цель данной части — продемонстрировать корректную работу очереди при одновременном доступе нескольких потоков.


## Что реализовано в данной части

1) Инициализация очереди
- Создаётся очередь фиксированной ёмкости `CAP`.
- Для хранения элементов используется массив в глобальной памяти GPU.
- Индексы `head` и `tail` инициализируются нулевыми значениями.

2) Параллельная операция enqueue
- Несколько GPU-потоков одновременно выполняют операцию `enqueue`.
- Каждый поток добавляет своё значение (идентификатор потока) в очередь.
- Переменная `tail` обновляется с помощью атомарной операции, что предотвращает конфликты записи.

3) Параллельная операция dequeue
- Несколько GPU-потоков одновременно выполняют операцию `dequeue`.
- Каждый поток извлекает один элемент из очереди в выходной массив.
- Атомарное обновление переменной `head` обеспечивает корректный порядок доступа к данным.

4) Проверка результата на CPU
После завершения работы GPU-ядер результаты копируются на CPU и выводятся:
- количество успешных операций `enqueue`
- количество успешных операций `dequeue`
- финальные значения индексов `head` и `tail`
- первые 10 извлечённых элементов очереди
