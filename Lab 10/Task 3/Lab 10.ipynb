{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ph5AutGvojjf",
        "outputId": "b6c956bb-5c27-467b-cb9c-758a66a90887"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting task1_omp.cpp\n"
          ]
        }
      ],
      "source": [
        "%%writefile task1_omp.cpp\n",
        "\n",
        "#include <iostream> // подключает cout\n",
        "#include <vector> // подключает vector\n",
        "#include <cstdlib> // подключает atoi\n",
        "#include <cmath> // подключает sqrt и fabs\n",
        "#include <omp.h> // подключает OpenMP\n",
        "#include <algorithm> // подключает max\n",
        "\n",
        "using namespace std; // чтобы не писать std::\n",
        "\n",
        "static void fill_array(vector<double>& a) { // заполняет массив\n",
        "    for (int i = 0; i < (int)a.size(); i++) { // цикл по массиву\n",
        "        a[i] = (double)(rand() % 1000) / 10.0; // кладёт число 0..99.9\n",
        "    }\n",
        "}\n",
        "\n",
        "static void calc_seq(const vector<double>& a, double& sum, double& sumsq) { // считает сумму и сумму квадратов последовательно\n",
        "    sum = 0.0; // обнуляет сумму\n",
        "    sumsq = 0.0; // обнуляет сумму квадратов\n",
        "    for (int i = 0; i < (int)a.size(); i++) { // цикл по массиву\n",
        "        double x = a[i]; // берёт элемент\n",
        "        sum += x; // прибавляет в сумму\n",
        "        sumsq += x * x; // прибавляет квадрат в сумму квадратов\n",
        "    }\n",
        "}\n",
        "\n",
        "static void calc_omp(const vector<double>& a, double& sum, double& sumsq) { // считает сумму и сумму квадратов параллельно\n",
        "    sum = 0.0; // обнуляет сумму\n",
        "    sumsq = 0.0; // обнуляет сумму квадратов\n",
        "#pragma omp parallel for reduction(+:sum,sumsq) // параллельный цикл с редукцией\n",
        "    for (int i = 0; i < (int)a.size(); i++) { // цикл по массиву\n",
        "        double x = a[i]; // берёт элемент\n",
        "        sum += x; // добавляет в общую сумму\n",
        "        sumsq += x * x; // добавляет в общую сумму квадратов\n",
        "    }\n",
        "}\n",
        "\n",
        "static double avg_time_seq(const vector<double>& a, int reps, double& sum, double& sumsq) { // меряет среднее время seq\n",
        "    double t = 0.0; // хранит суммарное время\n",
        "    for (int r = 0; r < reps; r++) { // повторяет несколько раз\n",
        "        double t0 = omp_get_wtime(); // старт времени\n",
        "        calc_seq(a, sum, sumsq); // запускает seq расчёт\n",
        "        double t1 = omp_get_wtime(); // конец времени\n",
        "        t += (t1 - t0); // добавляет длительность\n",
        "    }\n",
        "    return t / (double)max(1, reps); // возвращает среднее время\n",
        "}\n",
        "\n",
        "static double avg_time_omp(const vector<double>& a, int reps, double& sum, double& sumsq) { // меряет среднее время omp\n",
        "    double t = 0.0; // хранит суммарное время\n",
        "    for (int r = 0; r < reps; r++) { // повторяет несколько раз\n",
        "        double t0 = omp_get_wtime(); // старт времени\n",
        "        calc_omp(a, sum, sumsq); // запускает omp расчёт\n",
        "        double t1 = omp_get_wtime(); // конец времени\n",
        "        t += (t1 - t0); // добавляет длительность\n",
        "    }\n",
        "    return t / (double)max(1, reps); // возвращает среднее время\n",
        "}\n",
        "\n",
        "static double clamp01(double x) { // ограничивает число от 0 до 1\n",
        "    if (x < 0.0) return 0.0; // если ниже 0 то 0\n",
        "    if (x > 1.0) return 1.0; // если выше 1 то 1\n",
        "    return x; // иначе возвращает x\n",
        "}\n",
        "\n",
        "int main(int argc, char** argv) { // main\n",
        "    int N = 10000000; // размер массива\n",
        "    int threads = 0; // сколько потоков\n",
        "    int reps = 5; // сколько повторов\n",
        "    if (argc >= 2) N = atoi(argv[1]); // читает N из аргумента\n",
        "    if (argc >= 3) threads = atoi(argv[2]); // читает threads из аргумента\n",
        "    if (argc >= 4) reps = atoi(argv[3]); // читает reps из аргумента\n",
        "\n",
        "    if (threads > 0) omp_set_num_threads(threads); // задаёт число потоков\n",
        "    omp_set_dynamic(0); // запрещает OpenMP менять потоки сам\n",
        "\n",
        "    srand(123); // фиксирует seed\n",
        "    vector<double> a(N); // создаёт массив\n",
        "    fill_array(a); // заполняет массив\n",
        "\n",
        "    double warm_sum = 0.0; // переменная прогрева\n",
        "    double warm_sumsq = 0.0; // переменная прогрева\n",
        "    calc_seq(a, warm_sum, warm_sumsq); // прогрев seq\n",
        "    calc_omp(a, warm_sum, warm_sumsq); // прогрев omp\n",
        "\n",
        "    double seq_sum = 0.0; // сумма seq\n",
        "    double seq_sumsq = 0.0; // сумма квадратов seq\n",
        "    double omp_sum = 0.0; // сумма omp\n",
        "    double omp_sumsq = 0.0; // сумма квадратов omp\n",
        "\n",
        "    double t_seq = avg_time_seq(a, reps, seq_sum, seq_sumsq); // время seq\n",
        "    double t_omp = avg_time_omp(a, reps, omp_sum, omp_sumsq); // время omp\n",
        "\n",
        "    int used_threads = omp_get_max_threads(); // фактические потоки\n",
        "    double speedup = (t_omp > 0 ? t_seq / t_omp : 0.0); // ускорение\n",
        "\n",
        "    if (speedup > (double)used_threads) speedup = (double)used_threads; // защита от странных значений\n",
        "\n",
        "    double p = (double)max(1, used_threads); // p для Амдала\n",
        "    double s = (speedup > 1e-12 ? speedup : 1.0); // s для Амдала\n",
        "    double denom = (1.0 - 1.0 / p); // знаменатель\n",
        "    if (denom < 1e-12) denom = 1e-12; // защита\n",
        "    double f = ((1.0 / s) - (1.0 / p)) / denom; // оценивает долю последовательной части f\n",
        "    f = clamp01(f); // ограничивает f 0..1\n",
        "    double parallel_part = 1.0 - f; // оценивает долю параллельной части\n",
        "\n",
        "    double mean = omp_sum / (double)N; // считает среднее\n",
        "    double var = (omp_sumsq / (double)N) - mean * mean; // считает дисперсию\n",
        "    if (var < 0) var = 0; // защита от -0\n",
        "    double stddev = sqrt(var); // стандартное отклонение\n",
        "\n",
        "    double diff = fabs(seq_sum - omp_sum); // разница сумм\n",
        "    double rel = diff / max(1.0, fabs(seq_sum)); // относительная ошибка\n",
        "    bool ok_sum = (rel < 1e-9); // проверка корректности\n",
        "\n",
        "    cout << \"task 1\\n\"; // печатает метку\n",
        "    cout << \"n = \" << N << \"\\n\"; // печатает N\n",
        "    cout << \"threads = \" << used_threads << \"\\n\"; // печатает потоки\n",
        "    cout << \"seq ms = \" << (t_seq * 1000.0) << \"\\n\"; // печатает seq время\n",
        "    cout << \"omp ms = \" << (t_omp * 1000.0) << \"\\n\"; // печатает omp время\n",
        "    cout << \"speedup = \" << speedup << \"\\n\"; // печатает ускорение\n",
        "    cout << \"serial_part f = \" << f << \"\\n\"; // печатает последовательную долю\n",
        "    cout << \"parallel_part = \" << parallel_part << \"\\n\"; // печатает параллельную долю\n",
        "    cout << \"sum ok = \" << (ok_sum ? \"да\" : \"нет\") << \"\\n\"; // печатает проверку\n",
        "    cout << \"mean = \" << mean << \"\\n\"; // печатает mean\n",
        "    cout << \"var = \" << var << \"\\n\"; // печатает var\n",
        "    cout << \"std = \" << stddev << \"\\n\"; // печатает std\n",
        "    return 0; // завершает программу\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "g++ -O2 -fopenmp task1_omp.cpp -o task1_omp\n",
        "\n",
        "export OMP_PROC_BIND=true\n",
        "export OMP_PLACES=cores\n",
        "export OMP_DYNAMIC=false\n",
        "\n",
        "./task1_omp 10000000 1 5 | tee out1.txt\n",
        "./task1_omp 10000000 2 5 | tee out2.txt\n",
        "./task1_omp 10000000 4 5 | tee out4.txt\n",
        "./task1_omp 10000000 8 5 | tee out8.txt\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--66KBZJpbx9",
        "outputId": "2bfc96cf-15c8-48fe-f18d-b492be43f703"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "task 1\n",
            "n = 10000000\n",
            "threads = 1\n",
            "seq ms = 14.5836\n",
            "omp ms = 14.9502\n",
            "speedup = 0.975473\n",
            "serial_part f = 1\n",
            "parallel_part = 0\n",
            "sum ok = да\n",
            "mean = 49.9353\n",
            "var = 833.348\n",
            "std = 28.8678\n",
            "task 1\n",
            "n = 10000000\n",
            "threads = 2\n",
            "seq ms = 14.4841\n",
            "omp ms = 8.19282\n",
            "speedup = 1.7679\n",
            "serial_part f = 0.131284\n",
            "parallel_part = 0.868716\n",
            "sum ok = да\n",
            "mean = 49.9353\n",
            "var = 833.348\n",
            "std = 28.8678\n",
            "task 1\n",
            "n = 10000000\n",
            "threads = 4\n",
            "seq ms = 14.7065\n",
            "omp ms = 9.86706\n",
            "speedup = 1.49047\n",
            "serial_part f = 0.561241\n",
            "parallel_part = 0.438759\n",
            "sum ok = да\n",
            "mean = 49.9353\n",
            "var = 833.348\n",
            "std = 28.8678\n",
            "task 1\n",
            "n = 10000000\n",
            "threads = 8\n",
            "seq ms = 17.6666\n",
            "omp ms = 15.0418\n",
            "speedup = 1.17451\n",
            "serial_part f = 0.830196\n",
            "parallel_part = 0.169804\n",
            "sum ok = да\n",
            "mean = 49.9353\n",
            "var = 833.348\n",
            "std = 28.8678\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile task2_cuda_memory.cu\n",
        "\n",
        "#include <cuda_runtime.h> // подключает CUDA\n",
        "#include <iostream> // подключает cout\n",
        "#include <vector> // подключает vector\n",
        "#include <cstdlib> // подключает atoi\n",
        "#include <cmath> // подключает fabs\n",
        "\n",
        "using namespace std; // чтобы не писать std::\n",
        "\n",
        "static void cuda_ok(cudaError_t err, const char* msg) { // проверка CUDA\n",
        "    if (err != cudaSuccess) { // если ошибка\n",
        "        cout << \"CUDA error (\" << msg << \"): \" << cudaGetErrorString(err) << \"\\n\"; // выводит ошибку\n",
        "        exit(1); // завершает\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void k_coalesced(const float* in, float* out, int n) { // коалесцированный доступ\n",
        "    int i = blockIdx.x * blockDim.x + threadIdx.x; // индекс\n",
        "    if (i < n) out[i] = in[i] * 2.0f; // читает и пишет подряд\n",
        "}\n",
        "\n",
        "__device__ __forceinline__ int perm(int tid, int blockSize, int stride) { // перестановка индекса\n",
        "    return (tid * stride) & (blockSize - 1); // даёт \"прыгающий\" индекс внутри блока\n",
        "}\n",
        "\n",
        "__global__ void k_uncoalesced(const float* in, float* out, int n, int stride) { // некоалесцированный доступ\n",
        "    int base = blockIdx.x * blockDim.x; // база блока\n",
        "    int tid = threadIdx.x; // tid\n",
        "    int p = perm(tid, blockDim.x, stride); // переставляет tid\n",
        "    int i = base + p; // делает прыгающий индекс\n",
        "    if (i < n) out[i] = in[i] * 2.0f; // читает плохо по памяти\n",
        "}\n",
        "\n",
        "__global__ void k_shared_fix(const float* in, float* out, int n, int stride) { // оптимизация через shared\n",
        "    extern __shared__ float sh[]; // shared память\n",
        "    int base = blockIdx.x * blockDim.x; // база блока\n",
        "    int tid = threadIdx.x; // tid\n",
        "\n",
        "    int load_i = base + tid; // индекс чтения подряд\n",
        "    float x = 0.0f; // значение\n",
        "    if (load_i < n) x = in[load_i]; // читает коалесцированно\n",
        "    sh[tid] = x; // кладёт в shared\n",
        "    __syncthreads(); // синхронизация\n",
        "\n",
        "    int p = perm(tid, blockDim.x, stride); // перестановка\n",
        "    int out_i = base + p; // индекс записи\n",
        "    if (out_i < n) out[out_i] = sh[p] * 2.0f; // берёт из shared и пишет\n",
        "}\n",
        "\n",
        "static float time_kernel_coal(const float* d_in, float* d_out, int n, int blocks, int threads, int iters) { // время coalesced\n",
        "    cudaEvent_t s, e; // события\n",
        "    cuda_ok(cudaEventCreate(&s), \"event create\"); // создаёт start\n",
        "    cuda_ok(cudaEventCreate(&e), \"event create\"); // создаёт end\n",
        "    k_coalesced<<<blocks, threads>>>(d_in, d_out, n); // прогрев\n",
        "    cuda_ok(cudaDeviceSynchronize(), \"sync\"); // ждёт\n",
        "    cuda_ok(cudaEventRecord(s), \"record start\"); // старт\n",
        "    for (int i = 0; i < iters; i++) { // цикл итераций\n",
        "        k_coalesced<<<blocks, threads>>>(d_in, d_out, n); // запускает ядро\n",
        "    }\n",
        "    cuda_ok(cudaEventRecord(e), \"record end\"); // конец\n",
        "    cuda_ok(cudaEventSynchronize(e), \"sync end\"); // ждёт\n",
        "    float ms = 0.0f; // ms\n",
        "    cuda_ok(cudaEventElapsedTime(&ms, s, e), \"elapsed\"); // считает время\n",
        "    cuda_ok(cudaEventDestroy(s), \"destroy\"); // удаляет\n",
        "    cuda_ok(cudaEventDestroy(e), \"destroy\"); // удаляет\n",
        "    return ms / (float)max(1, iters); // среднее\n",
        "}\n",
        "\n",
        "static float time_kernel_bad(const float* d_in, float* d_out, int n, int blocks, int threads, int iters, int stride) { // время uncoalesced\n",
        "    cudaEvent_t s, e; // события\n",
        "    cuda_ok(cudaEventCreate(&s), \"event create\"); // start\n",
        "    cuda_ok(cudaEventCreate(&e), \"event create\"); // end\n",
        "    k_uncoalesced<<<blocks, threads>>>(d_in, d_out, n, stride); // прогрев\n",
        "    cuda_ok(cudaDeviceSynchronize(), \"sync\"); // ждёт\n",
        "    cuda_ok(cudaEventRecord(s), \"record start\"); // старт\n",
        "    for (int i = 0; i < iters; i++) { // цикл\n",
        "        k_uncoalesced<<<blocks, threads>>>(d_in, d_out, n, stride); // запускает ядро\n",
        "    }\n",
        "    cuda_ok(cudaEventRecord(e), \"record end\"); // конец\n",
        "    cuda_ok(cudaEventSynchronize(e), \"sync end\"); // ждёт\n",
        "    float ms = 0.0f; // ms\n",
        "    cuda_ok(cudaEventElapsedTime(&ms, s, e), \"elapsed\"); // время\n",
        "    cuda_ok(cudaEventDestroy(s), \"destroy\"); // удаляет\n",
        "    cuda_ok(cudaEventDestroy(e), \"destroy\"); // удаляет\n",
        "    return ms / (float)max(1, iters); // среднее\n",
        "}\n",
        "\n",
        "static float time_kernel_shared(const float* d_in, float* d_out, int n, int blocks, int threads, int iters, int stride) { // время shared_fix\n",
        "    cudaEvent_t s, e; // события\n",
        "    cuda_ok(cudaEventCreate(&s), \"event create\"); // start\n",
        "    cuda_ok(cudaEventCreate(&e), \"event create\"); // end\n",
        "    k_shared_fix<<<blocks, threads, threads * (int)sizeof(float)>>>(d_in, d_out, n, stride); // прогрев\n",
        "    cuda_ok(cudaDeviceSynchronize(), \"sync\"); // ждёт\n",
        "    cuda_ok(cudaEventRecord(s), \"record start\"); // старт\n",
        "    for (int i = 0; i < iters; i++) { // цикл\n",
        "        k_shared_fix<<<blocks, threads, threads * (int)sizeof(float)>>>(d_in, d_out, n, stride); // запускает ядро\n",
        "    }\n",
        "    cuda_ok(cudaEventRecord(e), \"record end\"); // конец\n",
        "    cuda_ok(cudaEventSynchronize(e), \"sync end\"); // ждёт\n",
        "    float ms = 0.0f; // ms\n",
        "    cuda_ok(cudaEventElapsedTime(&ms, s, e), \"elapsed\"); // время\n",
        "    cuda_ok(cudaEventDestroy(s), \"destroy\"); // удаляет\n",
        "    cuda_ok(cudaEventDestroy(e), \"destroy\"); // удаляет\n",
        "    return ms / (float)max(1, iters); // среднее\n",
        "}\n",
        "\n",
        "int main(int argc, char** argv) { // main\n",
        "    int N = 1 << 24; // размер\n",
        "    int iters = 50; // итерации\n",
        "    if (argc >= 2) N = atoi(argv[1]); // читает N\n",
        "    if (argc >= 3) iters = atoi(argv[2]); // читает iters\n",
        "\n",
        "    int threads = 256; // потоки\n",
        "    int blocks = (N + threads - 1) / threads; // блоки\n",
        "    int stride = 33; // stride для плохого доступа\n",
        "\n",
        "    vector<float> h_in(N); // host массив\n",
        "    for (int i = 0; i < N; i++) h_in[i] = (float)(i % 100); // заполняет\n",
        "\n",
        "    float* d_in = nullptr; // device in\n",
        "    float* d_out = nullptr; // device out\n",
        "    cuda_ok(cudaMalloc(&d_in, N * (int)sizeof(float)), \"malloc d_in\"); // malloc\n",
        "    cuda_ok(cudaMalloc(&d_out, N * (int)sizeof(float)), \"malloc d_out\"); // malloc\n",
        "    cuda_ok(cudaMemcpy(d_in, h_in.data(), N * (int)sizeof(float), cudaMemcpyHostToDevice), \"h2d\"); // копия\n",
        "\n",
        "    float ms_coal = time_kernel_coal(d_in, d_out, N, blocks, threads, iters); // время норм\n",
        "    float ms_bad = time_kernel_bad(d_in, d_out, N, blocks, threads, iters, stride); // время плохое\n",
        "    float ms_sh = time_kernel_shared(d_in, d_out, N, blocks, threads, iters, stride); // время shared\n",
        "\n",
        "    cout << \"task 2\\n\"; // метка\n",
        "    cout << \"n = \" << N << \"\\n\"; // n\n",
        "    cout << \"coalesced ms = \" << ms_coal << \"\\n\"; // вывод\n",
        "    cout << \"uncoalesced ms = \" << ms_bad << \"\\n\"; // вывод\n",
        "    cout << \"shared_fix ms = \" << ms_sh << \"\\n\"; // вывод\n",
        "\n",
        "    cuda_ok(cudaFree(d_in), \"free d_in\"); // free\n",
        "    cuda_ok(cudaFree(d_out), \"free d_out\"); // free\n",
        "    return 0; // выход\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bI9PkcthpehF",
        "outputId": "811bea69-97bf-47bf-b3c3-3b840aea0fb0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting task2_cuda_memory.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "nvcc -O2 -gencode arch=compute_75,code=sm_75 task2_cuda_memory.cu -o task2_cuda_memory\n",
        "./task2_cuda_memory 16777216 50\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lsfyp_aZpgr7",
        "outputId": "d1ac0681-4df4-4f65-cdf5-d44986b39c33"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "task 2\n",
            "n = 16777216\n",
            "coalesced ms = 0.56873\n",
            "uncoalesced ms = 0.595256\n",
            "shared_fix ms = 0.640637\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile task3_hybrid.cu\n",
        "\n",
        "#include <cuda_runtime.h> // подключает CUDA\n",
        "#include <iostream> // подключает cout\n",
        "#include <cstdlib> // подключает atoi\n",
        "#include <omp.h> // подключает omp_get_wtime\n",
        "\n",
        "using namespace std; // чтобы не писать std::\n",
        "\n",
        "static void cuda_ok(cudaError_t err, const char* msg) { // проверка CUDA\n",
        "    if (err != cudaSuccess) { // если ошибка\n",
        "        cout << \"CUDA error (\" << msg << \"): \" << cudaGetErrorString(err) << \"\\n\"; // выводит ошибку\n",
        "        exit(1); // завершает\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void mul2_kernel(float* a, int n) { // kernel умножает на 2\n",
        "    int i = blockIdx.x * blockDim.x + threadIdx.x; // индекс\n",
        "    if (i < n) a[i] *= 2.0f; // умножает\n",
        "}\n",
        "\n",
        "static void cpu_mul2(float* a, int n) { // CPU умножение\n",
        "    for (int i = 0; i < n; i++) a[i] *= 2.0f; // цикл\n",
        "}\n",
        "\n",
        "static float event_ms(cudaEvent_t s, cudaEvent_t e) { // считает время между событиями\n",
        "    float ms = 0.0f; // ms\n",
        "    cuda_ok(cudaEventElapsedTime(&ms, s, e), \"elapsed\"); // elapsed\n",
        "    return ms; // ms\n",
        "}\n",
        "\n",
        "int main(int argc, char** argv) { // main\n",
        "    int N = 10000000; // размер\n",
        "    int chunks = 2; // чанки GPU\n",
        "    if (argc >= 2) N = atoi(argv[1]); // читает N\n",
        "    if (argc >= 3) chunks = atoi(argv[2]); // читает chunks\n",
        "\n",
        "    int threads = 256; // потоки\n",
        "    int half = N / 2; // первая половина CPU\n",
        "    int n_gpu = N - half; // вторая половина GPU\n",
        "\n",
        "    float* h = nullptr; // pinned host память\n",
        "    cuda_ok(cudaMallocHost(&h, N * (int)sizeof(float)), \"mallocHost\"); // pinned alloc\n",
        "    for (int i = 0; i < N; i++) h[i] = (float)(i % 100); // заполняет\n",
        "\n",
        "    float* d = nullptr; // память GPU для второй половины\n",
        "    cuda_ok(cudaMalloc(&d, n_gpu * (int)sizeof(float)), \"cudaMalloc d\"); // alloc\n",
        "\n",
        "    cudaStream_t st1; // stream 1\n",
        "    cudaStream_t st2; // stream 2\n",
        "    cuda_ok(cudaStreamCreate(&st1), \"stream create\"); // create\n",
        "    cuda_ok(cudaStreamCreate(&st2), \"stream create\"); // create\n",
        "\n",
        "    cudaEvent_t e1; // событие start\n",
        "    cudaEvent_t e2; // событие end\n",
        "    cuda_ok(cudaEventCreate(&e1), \"event create\"); // create\n",
        "    cuda_ok(cudaEventCreate(&e2), \"event create\"); // create\n",
        "\n",
        "    cuda_ok(cudaEventRecord(e1, 0), \"record\"); // start h2d\n",
        "    cuda_ok(cudaMemcpy(d, h + half, n_gpu * (int)sizeof(float), cudaMemcpyHostToDevice), \"h2d\"); // sync copy\n",
        "    cuda_ok(cudaEventRecord(e2, 0), \"record\"); // end h2d\n",
        "    cuda_ok(cudaEventSynchronize(e2), \"sync\"); // sync\n",
        "    float h2d_only = event_ms(e1, e2); // время h2d\n",
        "\n",
        "    int blocks_gpu = (n_gpu + threads - 1) / threads; // блоки\n",
        "    cuda_ok(cudaEventRecord(e1, 0), \"record\"); // start kernel\n",
        "    mul2_kernel<<<blocks_gpu, threads>>>(d, n_gpu); // kernel\n",
        "    cuda_ok(cudaGetLastError(), \"kernel\"); // check\n",
        "    cuda_ok(cudaEventRecord(e2, 0), \"record\"); // end kernel\n",
        "    cuda_ok(cudaEventSynchronize(e2), \"sync\"); // sync\n",
        "    float k_only = event_ms(e1, e2); // время kernel\n",
        "\n",
        "    cuda_ok(cudaEventRecord(e1, 0), \"record\"); // start d2h\n",
        "    cuda_ok(cudaMemcpy(h + half, d, n_gpu * (int)sizeof(float), cudaMemcpyDeviceToHost), \"d2h\"); // sync copy\n",
        "    cuda_ok(cudaEventRecord(e2, 0), \"record\"); // end d2h\n",
        "    cuda_ok(cudaEventSynchronize(e2), \"sync\"); // sync\n",
        "    float d2h_only = event_ms(e1, e2); // время d2h\n",
        "\n",
        "    double t0 = omp_get_wtime(); // wall start\n",
        "\n",
        "#pragma omp parallel sections // две секции CPU и GPU\n",
        "    {\n",
        "#pragma omp section // GPU секция\n",
        "        {\n",
        "            int chunk = (n_gpu + chunks - 1) / chunks; // размер чанка\n",
        "            for (int c = 0; c < chunks; c++) { // цикл чанков\n",
        "                int off = c * chunk; // offset\n",
        "                int cur = chunk; // текущий размер\n",
        "                if (off + cur > n_gpu) cur = n_gpu - off; // обрезка\n",
        "                if (cur <= 0) continue; // защита\n",
        "\n",
        "                cudaStream_t st = (c % 2 == 0 ? st1 : st2); // выбирает стрим\n",
        "\n",
        "                cuda_ok(cudaMemcpyAsync(d + off, h + half + off, cur * (int)sizeof(float), cudaMemcpyHostToDevice, st), \"h2d async\"); // async h2d\n",
        "\n",
        "                int bl = (cur + threads - 1) / threads; // blocks\n",
        "                mul2_kernel<<<bl, threads, 0, st>>>(d + off, cur); // kernel в этом stream\n",
        "\n",
        "                cuda_ok(cudaMemcpyAsync(h + half + off, d + off, cur * (int)sizeof(float), cudaMemcpyDeviceToHost, st), \"d2h async\"); // async d2h\n",
        "            }\n",
        "\n",
        "            cuda_ok(cudaStreamSynchronize(st1), \"sync st1\"); // ждёт stream1\n",
        "            cuda_ok(cudaStreamSynchronize(st2), \"sync st2\"); // ждёт stream2\n",
        "        }\n",
        "\n",
        "#pragma omp section // CPU секция\n",
        "        {\n",
        "            cpu_mul2(h, half); // CPU обрабатывает первую половину\n",
        "        }\n",
        "    }\n",
        "\n",
        "    cuda_ok(cudaDeviceSynchronize(), \"device sync\"); // на всякий случай\n",
        "    double t1 = omp_get_wtime(); // wall end\n",
        "    double total_ms = (t1 - t0) * 1000.0; // total ms\n",
        "\n",
        "    cout << \"task 3\\n\"; // метка\n",
        "    cout << \"n = \" << N << \"\\n\"; // n\n",
        "    cout << \"chunks = \" << chunks << \"\\n\"; // chunks\n",
        "    cout << \"h2d only ms = \" << h2d_only << \"\\n\"; // h2d\n",
        "    cout << \"kernel only ms = \" << k_only << \"\\n\"; // kernel\n",
        "    cout << \"d2h only ms = \" << d2h_only << \"\\n\"; // d2h\n",
        "    cout << \"total hybrid ms = \" << total_ms << \"\\n\"; // total\n",
        "\n",
        "    cuda_ok(cudaEventDestroy(e1), \"event destroy\"); // destroy\n",
        "    cuda_ok(cudaEventDestroy(e2), \"event destroy\"); // destroy\n",
        "    cuda_ok(cudaStreamDestroy(st1), \"stream destroy\"); // destroy\n",
        "    cuda_ok(cudaStreamDestroy(st2), \"stream destroy\"); // destroy\n",
        "    cuda_ok(cudaFree(d), \"cudaFree\"); // free\n",
        "    cuda_ok(cudaFreeHost(h), \"cudaFreeHost\"); // free\n",
        "    return 0; // exit\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKkctG6nrB_0",
        "outputId": "37d56267-0cfc-4a2b-9faf-c3ecb499db35"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting task3_hybrid.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "nvcc -O2 -Xcompiler -fopenmp -gencode arch=compute_75,code=sm_75 task3_hybrid.cu -o task3_hybrid\n",
        "./task3_hybrid 10000000 1\n",
        "./task3_hybrid 10000000 2\n",
        "./task3_hybrid 10000000 4\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ynh3VyM4rFcA",
        "outputId": "87f89562-e2cf-4fa3-c6d9-e6afa5755379"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "task 3\n",
            "n = 10000000\n",
            "chunks = 1\n",
            "h2d only ms = 1.66934\n",
            "kernel only ms = 0.352736\n",
            "d2h only ms = 1.55309\n",
            "total hybrid ms = 11.4796\n",
            "task 3\n",
            "n = 10000000\n",
            "chunks = 2\n",
            "h2d only ms = 1.65635\n",
            "kernel only ms = 0.296736\n",
            "d2h only ms = 1.54227\n",
            "total hybrid ms = 4.1324\n",
            "task 3\n",
            "n = 10000000\n",
            "chunks = 4\n",
            "h2d only ms = 1.65587\n",
            "kernel only ms = 0.295168\n",
            "d2h only ms = 1.54262\n",
            "total hybrid ms = 4.08156\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile task4_mpi.cpp\n",
        "\n",
        "#include <mpi.h> // подключает MPI\n",
        "#include <iostream> // подключает cout\n",
        "#include <vector> // подключает vector\n",
        "#include <cstdlib> // подключает atoi\n",
        "#include <climits> // подключает INT_MAX INT_MIN\n",
        "\n",
        "using namespace std; // чтобы не писать std::\n",
        "\n",
        "static void fill_array(vector<int>& a) { // заполняет массив\n",
        "    srand(123); // seed\n",
        "    for (int i = 0; i < (int)a.size(); i++) { // цикл\n",
        "        a[i] = rand() % 1000; // кладёт 0..999\n",
        "    }\n",
        "}\n",
        "\n",
        "int main(int argc, char** argv) { // main\n",
        "    MPI_Init(&argc, &argv); // init MPI\n",
        "\n",
        "    int rank = 0; // rank\n",
        "    int size = 1; // size\n",
        "    MPI_Comm_rank(MPI_COMM_WORLD, &rank); // rank\n",
        "    MPI_Comm_size(MPI_COMM_WORLD, &size); // size\n",
        "\n",
        "    int mode = 0; // 0 strong, 1 weak\n",
        "    int Nstrong = 20000000; // N для strong\n",
        "    int Npp = 5000000; // N на процесс для weak\n",
        "    if (argc >= 2) mode = atoi(argv[1]); // mode\n",
        "    if (argc >= 3) Nstrong = atoi(argv[2]); // Nstrong\n",
        "    if (argc >= 4) Npp = atoi(argv[3]); // Npp\n",
        "\n",
        "    int globalN = (mode == 0 ? Nstrong : Npp * size); // итоговый N\n",
        "\n",
        "    vector<int> full; // полный массив\n",
        "    if (rank == 0) { // только root\n",
        "        full.assign(globalN, 0); // alloc\n",
        "        fill_array(full); // fill\n",
        "    }\n",
        "\n",
        "    vector<int> counts(size, 0); // counts\n",
        "    vector<int> displs(size, 0); // displs\n",
        "    int base = globalN / size; // база\n",
        "    int rem = globalN % size; // остаток\n",
        "    for (int r = 0; r < size; r++) { // цикл по ранкам\n",
        "        counts[r] = base + (r < rem ? 1 : 0); // раздаёт остаток\n",
        "    }\n",
        "    displs[0] = 0; // первое смещение\n",
        "    for (int r = 1; r < size; r++) { // цикл смещений\n",
        "        displs[r] = displs[r - 1] + counts[r - 1]; // суммирует\n",
        "    }\n",
        "\n",
        "    int local_n = counts[rank]; // локальный размер\n",
        "    vector<int> local(local_n); // локальный массив\n",
        "\n",
        "    MPI_Barrier(MPI_COMM_WORLD); // барьер\n",
        "    double t0 = MPI_Wtime(); // start time\n",
        "\n",
        "    MPI_Scatterv( // scatterv\n",
        "        rank == 0 ? full.data() : nullptr, // sendbuf\n",
        "        counts.data(), // sendcounts\n",
        "        displs.data(), // displs\n",
        "        MPI_INT, // тип\n",
        "        local.data(), // recvbuf\n",
        "        local_n, // recvcount\n",
        "        MPI_INT, // тип\n",
        "        0, // root\n",
        "        MPI_COMM_WORLD // comm\n",
        "    );\n",
        "\n",
        "    long long local_sum = 0; // local sum\n",
        "    int local_min = INT_MAX; // local min\n",
        "    int local_max = INT_MIN; // local max\n",
        "    for (int i = 0; i < local_n; i++) { // цикл\n",
        "        int x = local[i]; // x\n",
        "        local_sum += x; // sum\n",
        "        if (x < local_min) local_min = x; // min\n",
        "        if (x > local_max) local_max = x; // max\n",
        "    }\n",
        "\n",
        "    long long sum_reduce = 0; // reduce sum\n",
        "    int min_reduce = 0; // reduce min\n",
        "    int max_reduce = 0; // reduce max\n",
        "\n",
        "    MPI_Reduce(&local_sum, &sum_reduce, 1, MPI_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD); // reduce sum\n",
        "    MPI_Reduce(&local_min, &min_reduce, 1, MPI_INT, MPI_MIN, 0, MPI_COMM_WORLD); // reduce min\n",
        "    MPI_Reduce(&local_max, &max_reduce, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD); // reduce max\n",
        "\n",
        "    double t1 = MPI_Wtime(); // end time\n",
        "\n",
        "    long long sum_all = local_sum; // allreduce sum\n",
        "    int min_all = local_min; // allreduce min\n",
        "    int max_all = local_max; // allreduce max\n",
        "\n",
        "    MPI_Allreduce(MPI_IN_PLACE, &sum_all, 1, MPI_LONG_LONG, MPI_SUM, MPI_COMM_WORLD); // allreduce sum\n",
        "    MPI_Allreduce(MPI_IN_PLACE, &min_all, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD); // allreduce min\n",
        "    MPI_Allreduce(MPI_IN_PLACE, &max_all, 1, MPI_INT, MPI_MAX, MPI_COMM_WORLD); // allreduce max\n",
        "\n",
        "    if (rank == 0) { // печатает только root\n",
        "        cout << \"task 4\\n\"; // метка\n",
        "        cout << \"mode = \" << (mode == 0 ? \"strong\" : \"weak\") << \"\\n\"; // режим\n",
        "        cout << \"n = \" << globalN << \"\\n\"; // n\n",
        "        cout << \"np = \" << size << \"\\n\"; // np\n",
        "        cout << \"time reduce sec = \" << (t1 - t0) << \"\\n\"; // время reduce\n",
        "        cout << \"sum = \" << sum_reduce << \"\\n\"; // sum\n",
        "        cout << \"min = \" << min_reduce << \"\\n\"; // min\n",
        "        cout << \"max = \" << max_reduce << \"\\n\"; // max\n",
        "        cout << \"allreduce sum = \" << sum_all << \"\\n\"; // allreduce sum\n",
        "        cout << \"allreduce min = \" << min_all << \"\\n\"; // allreduce min\n",
        "        cout << \"allreduce max = \" << max_all << \"\\n\"; // allreduce max\n",
        "    }\n",
        "\n",
        "    MPI_Finalize(); // finalize\n",
        "    return 0; // exit\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0T8EWtRzrG_r",
        "outputId": "1df544fd-5610-4ebe-f0ed-33f9b66f0f7d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting task4_mpi.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "mpic++ -O2 task4_mpi.cpp -o task4_mpi\n",
        "\n",
        "# strong scaling (N фикс)\n",
        "mpirun --allow-run-as-root --oversubscribe -np 1 ./task4_mpi 0 20000000\n",
        "mpirun --allow-run-as-root --oversubscribe -np 2 ./task4_mpi 0 20000000\n",
        "mpirun --allow-run-as-root --oversubscribe -np 4 ./task4_mpi 0 20000000\n",
        "mpirun --allow-run-as-root --oversubscribe -np 8 ./task4_mpi 0 20000000\n",
        "\n",
        "# weak scaling (N растёт с np)\n",
        "mpirun --allow-run-as-root --oversubscribe -np 1 ./task4_mpi 1 0 5000000\n",
        "mpirun --allow-run-as-root --oversubscribe -np 2 ./task4_mpi 1 0 5000000\n",
        "mpirun --allow-run-as-root --oversubscribe -np 4 ./task4_mpi 1 0 5000000\n",
        "mpirun --allow-run-as-root --oversubscribe -np 8 ./task4_mpi 1 0 5000000\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHaWiUgxrI8o",
        "outputId": "83149624-a48e-48bc-f6dd-2f8cda5c6665"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "task 4\n",
            "mode = strong\n",
            "n = 20000000\n",
            "np = 1\n",
            "time reduce sec = 0.0380552\n",
            "sum = 9989562151\n",
            "min = 0\n",
            "max = 999\n",
            "allreduce sum = 9989562151\n",
            "allreduce min = 0\n",
            "allreduce max = 999\n"
          ]
        }
      ]
    }
  ]
}