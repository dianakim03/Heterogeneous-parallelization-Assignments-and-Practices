# Практическая работа №10  
Параллельные и гибридные вычисления: OpenMP, CUDA, MPI

## Цель работы
Изучение влияния параллелизма, доступа к памяти и коммуникаций на
производительность программ на CPU, GPU и в распределённой среде
с использованием технологий OpenMP, CUDA и MPI.

---

## Task 1. Анализ производительности CPU-параллельной программы (OpenMP)

### Что реализовано
- Последовательная и параллельная версии вычисления суммы, среднего значения,
  дисперсии и стандартного отклонения большого массива данных
- Параллелизация выполнена с использованием директивы OpenMP
  `parallel for reduction`
- Замеры времени выполнены с помощью `omp_get_wtime()`

### Что анализируется
- ускорение при увеличении числа потоков
- доля последовательной и параллельной части программы
- влияние числа потоков на эффективность
- оценка масштабируемости на основе закона Амдала

### Важное замечание
При небольшом числе потоков наблюдается ускорение, однако при дальнейшем
увеличении количества потоков производительность может снижаться из-за
накладных расходов OpenMP и ограниченной доли параллельного кода.
Это хорошо иллюстрирует практическое применение закона Амдала.

---

## Task 2. Оптимизация доступа к памяти на GPU (CUDA)

### Что реализовано
- CUDA-ядро с коалесцированным доступом к глобальной памяти
- CUDA-ядро с некоалесцированным (неэффективным) доступом к памяти
- Оптимизированная версия ядра с использованием разделяемой (shared) памяти

### Замеры производительности
- Время выполнения измеряется с помощью CUDA events (`cudaEvent`)
- Используется усреднение по нескольким итерациям для повышения точности

### Вывод
Производительность GPU-программы существенно зависит от паттерна доступа
к памяти. Даже при одинаковых вычислениях некоалесцированный доступ
приводит к снижению эффективности. Использование shared memory может
уменьшить влияние плохого доступа, но не всегда даёт выигрыш.

---

## Task 3. Профилирование гибридного приложения CPU + GPU

### Что реализовано
- Массив данных разделён на две части:
  - первая часть обрабатывается на CPU
  - вторая часть обрабатывается на GPU
- Используется pinned memory для ускорения копирования
- Применяются асинхронные передачи данных (`cudaMemcpyAsync`)
  и несколько CUDA stream

### Профилирование
- Отдельно измерены:
  - передача данных Host → Device (h2d)
  - выполнение CUDA-ядра
  - передача данных Device → Host (d2h)
- Замерен полный wall-time гибридного алгоритма

### Вывод
Гибридный подход даёт наибольший выигрыш при эффективном перекрытии
вычислений на CPU и GPU. Основным узким местом остаются накладные расходы
на передачу данных между CPU и GPU.

---

## Task 4. Анализ масштабируемости распределённой программы (MPI)

### Что реализовано
- Распределённое вычисление суммы, минимума и максимума массива
- Используется `MPI_Scatterv` для распределения данных
- Агрегация результатов выполняется с помощью `MPI_Reduce` и `MPI_Allreduce`
- Поддерживаются режимы strong scaling и weak scaling

### Анализ
- Измеряется общее время выполнения при разном числе процессов
- Оценивается влияние коммуникационных операций
- Сравнивается поведение Reduce и Allreduce

### Вывод
MPI-программы хорошо масштабируются при достаточном объёме вычислений
на процесс, однако при росте числа процессов коммуникационные издержки
начинают ограничивать ускорение.

---

## Общие выводы по работе
- Параллелизм эффективен только при достаточном размере задачи
- Для GPU доступ к памяти часто важнее самих вычислений
- В MPI-программах масштабируемость ограничивается коммуникациями
- Гибридные приложения чувствительны к накладным расходам передачи данных
